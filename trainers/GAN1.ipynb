{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "from scipy import ndimage\n",
    "import os\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow.contrib.layers as layers\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageset = np.empty((7704,64,64,3), dtype='float32')\n",
    "\n",
    "#There are 7704 images in the dataset.\n",
    "#214 folders. Each having 36 images.\n",
    "rootdir = '/home/chinmay/CODE/deep_learning/shapenet_datasets/mug_unprocessed/mug/models/3dw'\n",
    "cnt = 0\n",
    "total_cnt = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith('.png'):\n",
    "            if cnt == 36:\n",
    "                cnt = 0\n",
    "                total_cnt += 1\n",
    "            cnt += 1\n",
    "            seq_number = int(filepath.split('-')[-1].split('.')[0])\n",
    "            \n",
    "            imageset[total_cnt*36 + seq_number] = misc.imread(filepath).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train(data, pairs_per_model):\n",
    "    W,H,C = 64,64,3\n",
    "    images_per_model = 36\n",
    "    pose_W, pose_H, pose_D = 8,8,36\n",
    "\n",
    "    total_models = int(data.shape[0] / images_per_model)\n",
    "    print(total_models)\n",
    "    data_train = np.zeros((total_models * pairs_per_model, W,H,C))\n",
    "    \n",
    "    data_ = np.zeros((total_models * pairs_per_model,W,H,C))\n",
    "    labels_ = np.zeros((total_models * pairs_per_model,W,H,C))\n",
    "    pose = np.zeros((total_models * pairs_per_model,pose_W,pose_H,pose_D))\n",
    "    \n",
    "    for cnt in range(total_models ):\n",
    "        tmp_data = np.zeros((images_per_model * images_per_model,W,H,C))\n",
    "        tmp_labels = np.zeros((images_per_model * images_per_model,W,H,C))\n",
    "        tmp_pose = np.zeros((images_per_model * images_per_model, pose_W,pose_H,pose_D))\n",
    "        for i in range(images_per_model):\n",
    "            for j in range(images_per_model):\n",
    "                num = (j-i) % images_per_model\n",
    "                pose_tmp = np.zeros(images_per_model)\n",
    "                pose_tmp[num] = 1\n",
    "                tmp_pose[i*images_per_model + j] = np.broadcast_to(pose_tmp,(pose_W,pose_H,pose_D))\n",
    "                tmp_data[i*images_per_model + j] = data[cnt*images_per_model + i]\n",
    "                tmp_labels[i*images_per_model + j] = data[cnt *images_per_model + j]         \n",
    "        rand_nums = np.random.randint(0,images_per_model * images_per_model - 1,pairs_per_model)\n",
    "        data_[cnt*pairs_per_model : (cnt+1)*pairs_per_model] = tmp_data[rand_nums]\n",
    "        labels_[cnt*pairs_per_model : (cnt+1)*pairs_per_model] = tmp_labels[rand_nums]\n",
    "        pose[cnt*pairs_per_model : (cnt+1)*pairs_per_model] = tmp_pose[rand_nums]\n",
    "        \n",
    "    data_, labels_, pose = shuffle(data_, labels_, pose, random_state=0)\n",
    "    return data_, labels_, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    }
   ],
   "source": [
    "data_, labels_, pose = get_train(imageset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10700, 64, 64, 3)\n",
      "(10700, 64, 64, 3)\n",
      "(10700, 8, 8, 36)\n"
     ]
    }
   ],
   "source": [
    "print(data_.shape)\n",
    "print(labels_.shape)\n",
    "print(pose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 64, 64, 3)\n",
      "(9000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "data_ = np.zeros(imageset.shape)\n",
    "labels_ = np.zeros(imageset.shape)\n",
    "pose = np.zeros((7704,8,8,36))\n",
    "\n",
    "for cnt in range(214):\n",
    "    for itr in range(36):\n",
    "        data_[(cnt*36) + itr] = imageset[cnt*36]\n",
    "        pose_tmp = np.zeros(36)\n",
    "        pose_tmp[itr] = 1\n",
    "        pose[(cnt*36) + itr] = np.broadcast_to(pose_tmp,(8,8,36))\n",
    "        labels_[(cnt*36) + itr] =  imageset[(cnt*36) + itr]\n",
    "        #print( (cnt*36) + itr)\n",
    "\n",
    "data_, labels_, pose = shuffle(data_, labels_, pose, random_state=0)\n",
    "#This code will generate labels for the dataset\n",
    "#perm = np.random.permutation(data_.shape[0])\n",
    "\n",
    "# data_ = data_[perm]\n",
    "# labels_ = labels_[perm]\n",
    "# pose = pose[perm]\n",
    "'''\n",
    "# For current image label is the next image (image with 10 degree rotation)\n",
    "# For last image in the set (36th image) label would be first image\n",
    "data_train = np.array(data_[:9000])\n",
    "data_test = np.array(data_[9000:])\n",
    "\n",
    "train_labels = np.array(labels_[:9000])\n",
    "test_labels = np.array(labels_[9000:])\n",
    "\n",
    "print(data_train.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9378b0cda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f92f91e1b38>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = train_labels[16][...,::-1]\n",
    "tmp1 = data_train[16][...,::-1]\n",
    "\n",
    "print(pose[100][0][0])\n",
    "plt.imshow(tmp1/255)\n",
    "plt.show()\n",
    "plt.imshow(tmp/255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize data\n",
    "\n",
    "\n",
    "#data_train *= 2\n",
    "data_train /= 255\n",
    "#data_train -= 1\n",
    "\n",
    "#data_test *= 2\n",
    "data_test /= 255\n",
    "#data_test -= 1\n",
    "\n",
    "\n",
    "#train_labels *= 2\n",
    "train_labels /= 255\n",
    "#train_labels -= 1\n",
    "\n",
    "#test_labels *= 2\n",
    "test_labels /= 255\n",
    "#test_labels -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(input, output, kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv2d\", act=tf.nn.leaky_relu):\n",
    "    #act = tf.nn.leaky_relu\n",
    "    with tf.variable_scope(scope_name):\n",
    "        return layers.conv2d(inputs=input, num_outputs=output, kernel_size=[kernel_h, kernel_w], stride=k_stride, activation_fn=act,\n",
    "         biases_initializer=tf.zeros_initializer(), weights_initializer=tf.random_normal_initializer(0.0, 0.02))\n",
    "    \n",
    "def deconv2d(input, kernel_size, stride, num_filter, scope_name='deconv2d', act=tf.nn.leaky_relu):\n",
    "    with tf.variable_scope(scope_name): \n",
    "        stride_shape = [stride, stride]\n",
    "        kernel_shape = [kernel_size, kernel_size]\n",
    "        return layers.conv2d_transpose(inputs=input, num_outputs=num_filter, stride=stride_shape, kernel_size= kernel_shape,\n",
    "            padding='SAME', biases_initializer=tf.zeros_initializer(), weights_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            activation_fn=act\n",
    "        )\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "\n",
    "def batch_norm(inputs_, is_training):\n",
    "    out = tf.contrib.layers.batch_norm(inputs_, decay = 0.99, center = True, scale = True,\n",
    "                                       is_training = is_training, updates_collections = None)\n",
    "    return out\n",
    "\n",
    "def fc(input, num_output, name = 'fc'):\n",
    "    with tf.variable_scope(name):\n",
    "        num_input = input.get_shape()[1]\n",
    "        W = tf.get_variable('w', [num_input, num_output], tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [num_output], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input, W) + b\n",
    "\n",
    "def l1_loss(inputs_, targets_):\n",
    "    loss = tf.reduce_mean(abs(inputs_ - targets_))\n",
    "    return loss\n",
    "\n",
    "def ce_loss(labels, logits):\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "        return tf.reduce_mean(loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#to Reset Tensor Flow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "# tf Graph input (only pictures)\n",
    "inputs_ = tf.placeholder(tf.float32, (batch_size, 64,64,3), name=\"input\")\n",
    "targets_ = tf.placeholder(tf.float32, (batch_size, 64,64,3), name=\"target\")\n",
    "pose_ = tf.placeholder(tf.float32, (batch_size, 8, 8, 36), name=\"pose\")\n",
    "\n",
    "disc_inputs = tf.placeholder(tf.float32, (batch_size, 64,64,3), name=\"input\")\n",
    "\n",
    "real_label = tf.placeholder(tf.float32, [batch_size, 1])\n",
    "fake_label = tf.placeholder(tf.float32, [batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#generate coarse images from inputs_\n",
    "def auto_encoder(inputs_, pose_):\n",
    "    print(inputs_)\n",
    "    with tf.variable_scope('encoder'):\n",
    "        _ = conv2d(inputs_, output=16 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv1\",act=tf.nn.leaky_relu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = conv2d(_, output=32 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv2\",act=tf.nn.leaky_relu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = conv2d(_, output=92 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv3\",act=tf.nn.leaky_relu)\n",
    "        _ = tf.concat([_, pose_], 3)\n",
    "\n",
    "        _ = deconv2d(_, kernel_size=3, stride=2, num_filter=32, scope_name='deconv1',act=tf.nn.leaky_relu)\n",
    "        _ = deconv2d(_, kernel_size=3, stride=2, num_filter=16, scope_name='deconv2',act=tf.nn.leaky_relu)\n",
    "        _ = deconv2d(_, kernel_size=3, stride=2, num_filter=3, scope_name='deconv3',act=tf.nn.leaky_relu)\n",
    "        _ = tf.sigmoid(_)\n",
    "    return _ \n",
    "    \n",
    "    \n",
    "# discriminator should give a fc layer\n",
    "def discriminator(inputs_):\n",
    "    with tf.variable_scope('dis', reuse = tf.AUTO_REUSE):\n",
    "        ''' \n",
    "        _ = conv2d(inputs_, 4, 2, 32, 'conv1')\n",
    "        _ = leaky_relu(_)\n",
    "        _ = conv2d(_, 4, 2, 64, 'conv2')\n",
    "        _ = batch_norm(_)\n",
    "        _ = leaky_relu(_)\n",
    "        _ = conv2d(_, 4, 2, 128, 'conv3')\n",
    "        _ = batch_norm(_)\n",
    "        _ = leaky_relu(_)\n",
    "        _ = tf.reshape(_, [-1, 4 * 4 * 128])\n",
    "        _ = fc(_, 1, 'fc4')\n",
    "        print(_.get_shape(), \"FC - Result\")\n",
    "        '''\n",
    "        \n",
    "        _ = conv2d(inputs_, output=16 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"dis_conv1\", act=tf.nn.leaky_relu)\n",
    "        _ = conv2d(_, output=32 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"dis_conv2\", act=tf.nn.leaky_relu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = conv2d(_, output=32 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"dis_conv3\", act=tf.nn.leaky_relu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = tf.reshape(_, [-1, 8 * 8 * 32])        \n",
    "        _ = fc(_, 1, 'dis_fc1')\n",
    "        return _\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "lr = 5e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input:0\", shape=(25, 64, 64, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.9\n",
    "\n",
    "generated_images = auto_encoder(inputs_, pose_)\n",
    "dis_fake_samples_op = discriminator(generated_images)\n",
    "\n",
    "gen_loss_op = alpha * ce_loss(real_label, dis_fake_samples_op) + beta * l1_loss(generated_images, targets_)\n",
    "dis_loss_op = ce_loss(fake_label, dis_fake_samples_op) + ce_loss(real_label, discriminator(disc_inputs))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disriminator_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dis')\n",
    "generator_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'encoder')\n",
    "#print (generator_vars)\n",
    "dis_optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "dis_train_op = dis_optimizer.minimize(dis_loss_op, var_list=disriminator_vars)\n",
    "\n",
    "gen_optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "gen_train_op = gen_optimizer.minimize(gen_loss_op, var_list=generator_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: dis loss = 1.1756, gen loss = 0.3930\n",
      "Epoch 1: dis loss = 0.9858, gen loss = 0.2633\n",
      "Epoch 2: dis loss = 0.9521, gen loss = 0.2686\n",
      "Epoch 3: dis loss = 0.1491, gen loss = 0.6204\n",
      "Epoch 4: dis loss = 0.4917, gen loss = 0.7616\n",
      "Epoch 5: dis loss = 0.5520, gen loss = 0.6116\n",
      "Epoch 6: dis loss = 0.3650, gen loss = 0.4395\n",
      "Epoch 7: dis loss = 0.6677, gen loss = 0.5493\n",
      "Epoch 8: dis loss = 0.7540, gen loss = 0.3337\n",
      "Epoch 9: dis loss = 0.9926, gen loss = 0.3044\n",
      "Epoch 10: dis loss = 1.4167, gen loss = 0.3063\n",
      "Epoch 11: dis loss = 1.0680, gen loss = 0.2624\n",
      "Epoch 12: dis loss = 1.2637, gen loss = 0.4141\n",
      "Epoch 13: dis loss = 1.0946, gen loss = 0.2908\n",
      "Epoch 14: dis loss = 0.9902, gen loss = 0.4255\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-34e1170108e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m                          real_label: ones}\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 35\u001b[1;33m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mgen_train_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgen_loss_op\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgen_feed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    887\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    888\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 889\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    890\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    891\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1118\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1119\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1120\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1121\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1122\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1315\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1316\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[1;32m-> 1317\u001b[1;33m                            options, run_metadata)\n\u001b[0m\u001b[0;32m   1318\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1319\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1321\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1322\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1323\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1324\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1300\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[0;32m   1301\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1302\u001b[1;33m                                    status, run_metadata)\n\u001b[0m\u001b[0;32m   1303\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1304\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 50\n",
    "total_images = 9000\n",
    "step = 0\n",
    "\n",
    "r = total_images // batch_size\n",
    "#print(data_train.shape, \"X1\")\n",
    "#d_inputs = data_train[399*batch_size : (400)*batch_size]\n",
    "#print(d_inputs.shape, \"X2\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for ii in range(r):\n",
    "        step += 1\n",
    "        zeros = np.zeros([batch_size, 1])\n",
    "        ones = np.ones([batch_size, 1])\n",
    "        \n",
    "        batch = data_train[ii*batch_size : (ii+1)*batch_size]\n",
    "        batch_labels = train_labels[ii*batch_size : (ii+1)*batch_size]\n",
    "        pose_labels = pose[ii*batch_size : (ii+1)*batch_size]\n",
    "        #print(batch.shape, \"BS\")\n",
    "        #print(batch_labels.shape, \"LS\")\n",
    "        #print(pose_labels.shape)\n",
    "        \n",
    "        k = np.random.randint(1,r-1)\n",
    "        #print(\"K\", k)\n",
    "        \n",
    "        d_inputs = data_train[k*batch_size : (k+1)*batch_size]\n",
    "        \n",
    "        #print(d_inputs.shape, \"DI\", k)\n",
    "        gen_feed_dict = {inputs_: batch, targets_: batch_labels, pose_: pose_labels,\n",
    "                         real_label: ones}\n",
    "\n",
    "        _, gen_loss = sess.run([gen_train_op, gen_loss_op], feed_dict = gen_feed_dict)\n",
    "        \n",
    "        \n",
    "        dis_feed_dict = {inputs_: batch, targets_: batch_labels, pose_: pose_labels,\n",
    "                         fake_label: zeros, real_label: ones, disc_inputs: d_inputs}\n",
    "\n",
    "        _, dis_loss = sess.run([dis_train_op, dis_loss_op], feed_dict = dis_feed_dict)\n",
    "\n",
    "        #if step % 10 == 0:\n",
    "        #    print('Epoch {0}: dis loss = {1:.4f}, gen loss = {2:.4f}'.format(epoch, dis_loss, gen_loss))\n",
    "            \n",
    "    print('Epoch {0}: dis loss = {1:.4f}, gen loss = {2:.4f}'.format(epoch, dis_loss, gen_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dis')\n",
    "gen_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'encoder')\n",
    "saver = tf.train.Saver(dis_var_list + gen_var_list)\n",
    "saver.save(sess, 'model/gan1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "feed_dict = {inputs_: data_test[100:125], targets_: test_labels[100:125], pose_: pose[6120+100:6120+136],\n",
    "            }\n",
    "\n",
    "\n",
    "op = sess.run([generated_images], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = {inputs_: data_test[500:536], targets_: test_labels[500:536], pose_:pose[6120+500:6120+536],\n",
    "            }\n",
    "\n",
    "\n",
    "op = sess.run([generated_images], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(op[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = op[0][22]\n",
    "print(tmp.shape)\n",
    "\n",
    "#print(np.max(tmp), np.min(tmp))\n",
    "plt.imshow(tmp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_labels[500:536][22])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
