{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import misc\n",
    "from scipy import ndimage\n",
    "import os\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.image as mpimg\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow.contrib.layers as layers\n",
    "import tensorflow.contrib.slim as slim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "imageset = np.empty((7704,64,64,3), dtype='float32')\n",
    "\n",
    "#There are 7704 images in the dataset.\n",
    "#214 folders. Each having 36 images.\n",
    "rootdir = '/home/chinmay/CODE/deep_learning/shapenet_datasets/mug_unprocessed/mug/models/3dw'\n",
    "cnt = 0\n",
    "total_cnt = 0\n",
    "\n",
    "for subdir, dirs, files in os.walk(rootdir):\n",
    "    for file in files:\n",
    "        filepath = subdir + os.sep + file\n",
    "        if filepath.endswith('.png'):\n",
    "            if cnt == 36:\n",
    "                cnt = 0\n",
    "                total_cnt += 1\n",
    "            cnt += 1\n",
    "            seq_number = int(filepath.split('-')[-1].split('.')[0])\n",
    "            \n",
    "            imageset[total_cnt*36 + seq_number] = misc.imread(filepath).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_train(data, pairs_per_model):\n",
    "    W,H,C = 64,64,3\n",
    "    images_per_model = 36\n",
    "    pose_W, pose_H, pose_D = 8,8,36\n",
    "\n",
    "    total_models = int(data.shape[0] / images_per_model)\n",
    "    print(total_models)\n",
    "    data_train = np.zeros((total_models * pairs_per_model, W,H,C))\n",
    "    \n",
    "    data_ = np.zeros((total_models * pairs_per_model,W,H,C))\n",
    "    labels_ = np.zeros((total_models * pairs_per_model,W,H,C))\n",
    "    pose = np.zeros((total_models * pairs_per_model,pose_W,pose_H,pose_D))\n",
    "    \n",
    "    for cnt in range(total_models ):\n",
    "        tmp_data = np.zeros((images_per_model * images_per_model,W,H,C))\n",
    "        tmp_labels = np.zeros((images_per_model * images_per_model,W,H,C))\n",
    "        tmp_pose = np.zeros((images_per_model * images_per_model, pose_W,pose_H,pose_D))\n",
    "        for i in range(images_per_model):\n",
    "            for j in range(images_per_model):\n",
    "                num = (j-i) % images_per_model\n",
    "                pose_tmp = np.zeros(images_per_model)\n",
    "                pose_tmp[num] = 1\n",
    "                tmp_pose[i*images_per_model + j] = np.broadcast_to(pose_tmp,(pose_W,pose_H,pose_D))\n",
    "                tmp_data[i*images_per_model + j] = data[cnt*images_per_model + i]\n",
    "                tmp_labels[i*images_per_model + j] = data[cnt *images_per_model + j]         \n",
    "        rand_nums = np.random.randint(0,images_per_model * images_per_model - 1,pairs_per_model)\n",
    "        data_[cnt*pairs_per_model : (cnt+1)*pairs_per_model] = tmp_data[rand_nums]\n",
    "        labels_[cnt*pairs_per_model : (cnt+1)*pairs_per_model] = tmp_labels[rand_nums]\n",
    "        pose[cnt*pairs_per_model : (cnt+1)*pairs_per_model] = tmp_pose[rand_nums]\n",
    "        \n",
    "    data_, labels_, pose = shuffle(data_, labels_, pose, random_state=0)\n",
    "    return data_, labels_, pose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "214\n"
     ]
    }
   ],
   "source": [
    "data_, labels_, pose = get_train(imageset, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10700, 64, 64, 3)\n",
      "(10700, 64, 64, 3)\n",
      "(10700, 8, 8, 36)\n"
     ]
    }
   ],
   "source": [
    "print(data_.shape)\n",
    "print(labels_.shape)\n",
    "print(pose.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9000, 64, 64, 3)\n",
      "(9000, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "data_ = np.zeros(imageset.shape)\n",
    "labels_ = np.zeros(imageset.shape)\n",
    "pose = np.zeros((7704,8,8,36))\n",
    "\n",
    "for cnt in range(214):\n",
    "    for itr in range(36):\n",
    "        data_[(cnt*36) + itr] = imageset[cnt*36]\n",
    "        pose_tmp = np.zeros(36)\n",
    "        pose_tmp[itr] = 1\n",
    "        pose[(cnt*36) + itr] = np.broadcast_to(pose_tmp,(8,8,36))\n",
    "        labels_[(cnt*36) + itr] =  imageset[(cnt*36) + itr]\n",
    "        #print( (cnt*36) + itr)\n",
    "\n",
    "data_, labels_, pose = shuffle(data_, labels_, pose, random_state=0)\n",
    "#This code will generate labels for the dataset\n",
    "#perm = np.random.permutation(data_.shape[0])\n",
    "\n",
    "# data_ = data_[perm]\n",
    "# labels_ = labels_[perm]\n",
    "# pose = pose[perm]\n",
    "'''\n",
    "# For current image label is the next image (image with 10 degree rotation)\n",
    "# For last image in the set (36th image) label would be first image\n",
    "data_train = np.array(data_[:9000])\n",
    "data_test = np.array(data_[9000:])\n",
    "\n",
    "train_labels = np.array(labels_[:9000])\n",
    "test_labels = np.array(labels_[9000:])\n",
    "\n",
    "print(data_train.shape)\n",
    "print(train_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac72c61ac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP4AAAD8CAYAAABXXhlaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQU58swT9P4sCixP0SeUNAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fabdc9b86d8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tmp = train_labels[16][...,::-1]\n",
    "tmp1 = data_train[16][...,::-1]\n",
    "\n",
    "print(pose[100][0][0])\n",
    "plt.imshow(tmp1/255)\n",
    "plt.show()\n",
    "plt.imshow(tmp/255)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#normalize data\n",
    "\n",
    "\n",
    "#data_train *= 2\n",
    "data_train /= 255\n",
    "#data_train -= 1\n",
    "\n",
    "#data_test *= 2\n",
    "data_test /= 255\n",
    "#data_test -= 1\n",
    "\n",
    "\n",
    "#train_labels *= 2\n",
    "train_labels /= 255\n",
    "#train_labels -= 1\n",
    "\n",
    "#test_labels *= 2\n",
    "test_labels /= 255\n",
    "#test_labels -= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conv2d(input, output, kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv2d\", act=tf.nn.leaky_relu):\n",
    "    #act = tf.nn.leaky_relu\n",
    "    with tf.variable_scope(scope_name):\n",
    "        return layers.conv2d(inputs=input, num_outputs=output, kernel_size=[kernel_h, kernel_w], stride=k_stride, activation_fn=act,\n",
    "         biases_initializer=tf.zeros_initializer(), weights_initializer=tf.random_normal_initializer(0.0, 0.02))\n",
    "    \n",
    "def deconv2d(input, kernel_size, stride, num_filter, scope_name='deconv2d', act=tf.nn.leaky_relu):\n",
    "    with tf.variable_scope(scope_name): \n",
    "        stride_shape = [stride, stride]\n",
    "        kernel_shape = [kernel_size, kernel_size]\n",
    "        return layers.conv2d_transpose(inputs=input, num_outputs=num_filter, stride=stride_shape, kernel_size= kernel_shape,\n",
    "            padding='SAME', biases_initializer=tf.zeros_initializer(), weights_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            activation_fn=act\n",
    "        )\n",
    "\n",
    "def max_pool(input, kernel_size, stride):\n",
    "    ksize = [1, kernel_size, kernel_size, 1]\n",
    "    strides = [1, stride, stride, 1]\n",
    "    return tf.nn.max_pool(input, ksize=ksize, strides=strides, padding='SAME')\n",
    "\n",
    "\n",
    "def batch_norm(inputs_, is_training):\n",
    "    out = tf.contrib.layers.batch_norm(inputs_, decay = 0.99, center = True, scale = True,\n",
    "                                       is_training = is_training, updates_collections = None)\n",
    "    return out\n",
    "\n",
    "def fc(input, num_output, name = 'fc'):\n",
    "    with tf.variable_scope(name):\n",
    "        num_input = input.get_shape()[1]\n",
    "        W = tf.get_variable('w', [num_input, num_output], tf.float32, tf.random_normal_initializer(0.0, 0.02))\n",
    "        b = tf.get_variable('b', [num_output], initializer = tf.constant_initializer(0.0))\n",
    "        return tf.matmul(input, W) + b\n",
    "\n",
    "def l1_loss(inputs_, targets_):\n",
    "    loss = tf.reduce_mean(abs(inputs_ - targets_))\n",
    "    return loss\n",
    "\n",
    "def ce_loss(labels, logits):\n",
    "        loss = tf.nn.sigmoid_cross_entropy_with_logits(labels = labels, logits = logits)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "def lrelu(x, leak=0.2, name=\"lrelu\"):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "    return f1 * x + f2 * abs(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "#to Reset Tensor Flow Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "# tf Graph input (only pictures)\n",
    "inputs_ = tf.placeholder(tf.float32, (batch_size, 64,64,3), name=\"input\")\n",
    "targets_ = tf.placeholder(tf.float32, (batch_size, 64,64,3), name=\"target\")\n",
    "pose_ = tf.placeholder(tf.float32, (batch_size, 8, 8, 36), name=\"pose\")\n",
    "\n",
    "disc_inputs = tf.placeholder(tf.float32, (batch_size, 64,64,3), name=\"input\")\n",
    "\n",
    "real_label = tf.placeholder(tf.float32, [batch_size, 1])\n",
    "fake_label = tf.placeholder(tf.float32, [batch_size, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cur_reuse = False\n",
    "\n",
    "\n",
    "#generate coarse images from inputs_\n",
    "def auto_encoder(inputs_, pose_):\n",
    "    print(inputs_)\n",
    "    with tf.variable_scope('encoder', reuse=True):\n",
    "        _ = conv2d(inputs_, output=16 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv1\",act=lrelu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = conv2d(_, output=32 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv2\",act=lrelu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = conv2d(_, output=92 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"conv3\",act=lrelu)\n",
    "        _ = tf.concat([_, pose_], 3)\n",
    "\n",
    "        _ = deconv2d(_, kernel_size=3, stride=2, num_filter=32, scope_name='deconv1',act=lrelu)\n",
    "        _ = deconv2d(_, kernel_size=3, stride=2, num_filter=16, scope_name='deconv2',act=lrelu)\n",
    "        _ = deconv2d(_, kernel_size=3, stride=2, num_filter=3, scope_name='deconv3',act=lrelu)\n",
    "        _ = tf.sigmoid(_)\n",
    "    return _ \n",
    "    \n",
    "    \n",
    "# discriminator should give a fc layer\n",
    "def discriminator(inputs_, r):\n",
    "    with tf.variable_scope('dis', reuse=r):\n",
    "        #cur_reuse = True\n",
    "        ''' \n",
    "        _ = conv2d(inputs_, 4, 2, 32, 'conv1')\n",
    "        _ = leaky_relu(_)\n",
    "        _ = conv2d(_, 4, 2, 64, 'conv2')\n",
    "        _ = batch_norm(_)\n",
    "        _ = leaky_relu(_)\n",
    "        _ = conv2d(_, 4, 2, 128, 'conv3')\n",
    "        _ = batch_norm(_)\n",
    "        _ = leaky_relu(_)\n",
    "        _ = tf.reshape(_, [-1, 4 * 4 * 128])\n",
    "        _ = fc(_, 1, 'fc4')\n",
    "        print(_.get_shape(), \"FC - Result\")\n",
    "        '''\n",
    "        \n",
    "        _ = conv2d(inputs_, output=16 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"dis_conv1\", act=lrelu)\n",
    "        _ = conv2d(_, output=32 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"dis_conv2\", act=lrelu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = conv2d(_, output=32 ,kernel_h=3, kernel_w=3, k_stride=2, scope_name=\"dis_conv3\", act=lrelu)\n",
    "        _ = batch_norm(_, True)\n",
    "        _ = tf.reshape(_, [-1, 8 * 8 * 32])        \n",
    "        _ = fc(_, 1, 'dis_fc1')\n",
    "        return _\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "global_step = tf.Variable(0,trainable=False)\n",
    "lr = 5e-4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"input:0\", shape=(25, 64, 64, 3), dtype=float32)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Variable encoder/conv1/Conv/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 217, in variable\n    use_resource=use_resource)\n  File \"/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 262, in model_variable\n    use_resource=use_resource)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-548c83baaa23>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbeta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.9\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mgenerated_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mauto_encoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpose_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mdis_fake_samples_op\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdiscriminator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerated_images\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-25-686fa05fcc78>\u001b[0m in \u001b[0;36mauto_encoder\u001b[1;34m(inputs_, pose_)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'encoder'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m16\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mkernel_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_stride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"conv1\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m32\u001b[0m \u001b[1;33m,\u001b[0m\u001b[0mkernel_h\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_w\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mk_stride\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"conv2\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlrelu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-7a4b64be0850>\u001b[0m in \u001b[0;36mconv2d\u001b[1;34m(input, output, kernel_h, kernel_w, k_stride, scope_name, act)\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         return layers.conv2d(inputs=input, num_outputs=output, kernel_size=[kernel_h, kernel_w], stride=k_stride, activation_fn=act,\n\u001b[1;32m----> 5\u001b[1;33m          biases_initializer=tf.zeros_initializer(), weights_initializer=tf.random_normal_initializer(0.0, 0.02))\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mdeconv2d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_filter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscope_name\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'deconv2d'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mleaky_relu\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mconvolution\u001b[1;34m(inputs, num_outputs, kernel_size, stride, padding, data_format, rate, activation_fn, normalizer_fn, normalizer_params, weights_initializer, weights_regularizer, biases_initializer, biases_regularizer, reuse, variables_collections, outputs_collections, trainable, scope)\u001b[0m\n\u001b[0;32m   1031\u001b[0m                         \u001b[0m_scope\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1032\u001b[0m                         _reuse=reuse)\n\u001b[1;32m-> 1033\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1034\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m     \u001b[1;31m# Add variables to collections.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    669\u001b[0m       \u001b[0mOutput\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    670\u001b[0m     \"\"\"\n\u001b[1;32m--> 671\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    672\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    673\u001b[0m   def _add_inbound_node(self,\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    557\u001b[0m           \u001b[0minput_shapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minput_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m           \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    561\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shapes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/layers/convolutional.py\u001b[0m in \u001b[0;36mbuild\u001b[1;34m(self, input_shape)\u001b[0m\n\u001b[0;32m    141\u001b[0m                                     \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkernel_constraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m                                     \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m                                     dtype=self.dtype)\n\u001b[0m\u001b[0;32m    144\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse_bias\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    145\u001b[0m       self.bias = self.add_variable(name='bias',\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36madd_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, trainable, constraint)\u001b[0m\n\u001b[0;32m    456\u001b[0m                                    \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m                                    \u001b[0mconstraint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m                                    trainable=trainable and self.trainable)\n\u001b[0m\u001b[0;32m    459\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvariable\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexisting_variables\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mvariable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1201\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1202\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1203\u001b[1;33m       constraint=constraint)\n\u001b[0m\u001b[0;32m   1204\u001b[0m get_variable_or_local_docstring = (\n\u001b[0;32m   1205\u001b[0m     \"\"\"%s\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m   1090\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1091\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1092\u001b[1;33m           constraint=constraint)\n\u001b[0m\u001b[0;32m   1093\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1094\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[0;32m    415\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;34m\"constraint\"\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    416\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"constraint\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 417\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    418\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    419\u001b[0m       return _true_getter(\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36mlayer_variable_getter\u001b[1;34m(getter, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1537\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1538\u001b[0m     \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'rename'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrename\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1539\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_model_variable_getter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1540\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mlayer_variable_getter\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1541\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/layers/python/layers/layers.py\u001b[0m in \u001b[0;36m_model_variable_getter\u001b[1;34m(getter, name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, rename, use_resource, **_)\u001b[0m\n\u001b[0;32m   1529\u001b[0m       \u001b[0mregularizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mregularizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1530\u001b[0m       \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1531\u001b[1;33m       custom_getter=getter, use_resource=use_resource)\n\u001b[0m\u001b[0;32m   1532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1533\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mmodel_variable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource)\u001b[0m\n\u001b[0;32m    260\u001b[0m                  \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m                  \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m                  use_resource=use_resource)\n\u001b[0m\u001b[0;32m    263\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mvar\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\u001b[0m in \u001b[0;36mfunc_with_args\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    179\u001b[0m       \u001b[0mcurrent_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcurrent_scope\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m       \u001b[0mcurrent_args\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcurrent_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m   \u001b[0m_add_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m   \u001b[0msetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc_with_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'_key_op'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_key_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\u001b[0m in \u001b[0;36mvariable\u001b[1;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, device, partitioner, custom_getter, use_resource)\u001b[0m\n\u001b[0;32m    215\u001b[0m                   \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m                   \u001b[0mpartitioner\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m                   use_resource=use_resource)\n\u001b[0m\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[1;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    392\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[1;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[0;32m    740\u001b[0m                          \u001b[1;34m\"reuse=tf.AUTO_REUSE in VarScope? \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m                          \"Originally defined at:\\n\\n%s\" % (\n\u001b[1;32m--> 742\u001b[1;33m                              name, \"\".join(traceback.format_list(tb))))\n\u001b[0m\u001b[0;32m    743\u001b[0m       \u001b[0mfound_var\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_vars\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfound_var\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Variable encoder/conv1/Conv/weights already exists, disallowed. Did you mean to set reuse=True or reuse=tf.AUTO_REUSE in VarScope? Originally defined at:\n\n  File \"/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 217, in variable\n    use_resource=use_resource)\n  File \"/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/arg_scope.py\", line 181, in func_with_args\n    return func(*args, **current_args)\n  File \"/home/chinmay/CODE/deep_learning/599-project/repo/Multi-Viewpoint-Image-generation/multi_venv/lib/python3.5/site-packages/tensorflow/contrib/framework/python/ops/variables.py\", line 262, in model_variable\n    use_resource=use_resource)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "beta = 0.9\n",
    "\n",
    "generated_images = auto_encoder(inputs_, pose_, r=False)\n",
    "dis_fake_samples_op = discriminator(generated_images,r=False)\n",
    "\n",
    "gen_loss_op = alpha * ce_loss(real_label, dis_fake_samples_op) + beta * l1_loss(generated_images, targets_)\n",
    "dis_loss_op = ce_loss(fake_label, dis_fake_samples_op) + ce_loss(real_label, discriminator(disc_inputs,r=True))\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disriminator_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dis')\n",
    "generator_vars = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'encoder')\n",
    "#print (generator_vars)\n",
    "dis_optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "dis_train_op = dis_optimizer.minimize(dis_loss_op, var_list=disriminator_vars)\n",
    "\n",
    "gen_optimizer = tf.train.RMSPropOptimizer(lr)\n",
    "gen_train_op = gen_optimizer.minimize(gen_loss_op, var_list=generator_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "epochs = 50\n",
    "total_images = 9000\n",
    "step = 0\n",
    "\n",
    "r = total_images // batch_size\n",
    "#print(data_train.shape, \"X1\")\n",
    "#d_inputs = data_train[399*batch_size : (400)*batch_size]\n",
    "#print(d_inputs.shape, \"X2\")\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for ii in range(r):\n",
    "        step += 1\n",
    "        zeros = np.zeros([batch_size, 1])\n",
    "        ones = np.ones([batch_size, 1])\n",
    "        \n",
    "        batch = data_train[ii*batch_size : (ii+1)*batch_size]\n",
    "        batch_labels = train_labels[ii*batch_size : (ii+1)*batch_size]\n",
    "        pose_labels = pose[ii*batch_size : (ii+1)*batch_size]\n",
    "        #print(batch.shape, \"BS\")\n",
    "        #print(batch_labels.shape, \"LS\")\n",
    "        #print(pose_labels.shape)\n",
    "        \n",
    "        k = np.random.randint(1,r-1)\n",
    "        #print(\"K\", k)\n",
    "        \n",
    "        d_inputs = data_train[k*batch_size : (k+1)*batch_size]\n",
    "        \n",
    "        #print(d_inputs.shape, \"DI\", k)\n",
    "        gen_feed_dict = {inputs_: batch, targets_: batch_labels, pose_: pose_labels,\n",
    "                         real_label: ones}\n",
    "\n",
    "        _, gen_loss = sess.run([gen_train_op, gen_loss_op], feed_dict = gen_feed_dict)\n",
    "        \n",
    "        \n",
    "        dis_feed_dict = {inputs_: batch, targets_: batch_labels, pose_: pose_labels,\n",
    "                         fake_label: zeros, real_label: ones, disc_inputs: d_inputs}\n",
    "\n",
    "        _, dis_loss = sess.run([dis_train_op, dis_loss_op], feed_dict = dis_feed_dict)\n",
    "\n",
    "        #if step % 10 == 0:\n",
    "        #    print('Epoch {0}: dis loss = {1:.4f}, gen loss = {2:.4f}'.format(epoch, dis_loss, gen_loss))\n",
    "            \n",
    "    print('Epoch {0}: dis loss = {1:.4f}, gen loss = {2:.4f}'.format(epoch, dis_loss, gen_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dis_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'dis')\n",
    "gen_var_list = tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, 'encoder')\n",
    "saver = tf.train.Saver(dis_var_list + gen_var_list)\n",
    "saver.save(sess, 'model/gan1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "feed_dict = {inputs_: data_test[100:125], targets_: test_labels[100:125], pose_: pose[6120+100:6120+136],\n",
    "            }\n",
    "\n",
    "\n",
    "op = sess.run([generated_images], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feed_dict = {inputs_: data_test[500:536], targets_: test_labels[500:536], pose_:pose[6120+500:6120+536],\n",
    "            }\n",
    "\n",
    "\n",
    "op = sess.run([generated_images], feed_dict = feed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(op[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tmp = op[0][22]\n",
    "print(tmp.shape)\n",
    "\n",
    "#print(np.max(tmp), np.min(tmp))\n",
    "plt.imshow(tmp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.imshow(test_labels[500:536][22])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
